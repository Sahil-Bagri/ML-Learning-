<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Engineering & Selection Knowledge Base</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Feature Engineering & Selection</h1>
            <p class="subtitle">Knowledge Base with Text-to-Speech</p>
        </header>
        
        <div class="content">
            <div class="selection-area">
                <label for="question-select">Select a Topic:</label>
                <select id="question-select">
                    <option value="">-- Choose a topic --</option>
                    <option value="1">1. Feature Engineering</option>
                    <option value="2">2. Feature Selection</option>
                    <option value="3">3. Filter Based Approach</option>
                    <option value="4">4. Information Gain</option>
                    <option value="5">5. Chi-Square Test</option>
                    <option value="6">6. Fisher Score</option>
                    <option value="7">7. Handling Missing Values</option>
                    <option value="8">8. Wrapper Method</option>
                    <option value="9">9. Exhaustive Feature Selection</option>
                    <option value="10">10. Recursive Feature Elimination (RFE)</option>
                    <option value="11">11. Project: Titanic Dataset Feature Engineering</option>
                    <option value="12">12. Encoding Categorical Variables</option>
                    <option value="13">13. Embedding</option>
                    <option value="14">14. Dimensionality Reduction</option>
                    <option value="15">15. Principal Component Analysis (PCA)</option>
                    <option value="16">16. Linear Discriminant Analysis (LDA)</option>
                    <option value="17">17. Bagging and Boosting</option>
                </select>
            </div>
            
            <div class="answer-area">
                <div class="answer-title" id="answer-title">Please select a topic to view its explanation</div>
                <div class="answer-content" id="answer-content"></div>
            </div>
            
            <div class="controls">
                <button class="speak-btn" id="speak-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                        <path d="M11.536 14.01A8.473 8.473 0 0 0 14.026 8a8.473 8.473 0 0 0-2.49-6.01l-.708.707A7.476 7.476 0 0 1 13.025 8c0 2.071-.84 3.946-2.197 5.303l.708.707z"/>
                        <path d="M10.121 12.596A6.48 6.48 0 0 0 12.025 8a6.48 6.48 0 0 0-1.904-4.596l-.707.707A5.483 5.483 0 0 1 11.025 8a5.483 5.483 0 0 1-1.61 3.89l.706.706z"/>
                        <path d="M8.707 11.182A4.486 4.486 0 0 0 10.025 8a4.486 4.486 0 0 0-1.318-3.182L8 5.525A3.489 3.489 0 0 1 9.025 8 3.49 3.49 0 0 1 8 10.475l.707.707zM6.717 3.55A.5.5 0 0 1 7.43 4.084L5.64 5.872a2.5 2.5 0 0 0-.708 1.064L4.78 8.048a.5.5 0 0 1-.932-.372l.61-3.123a3.5 3.5 0 0 1 .992-1.51L6.717 3.55z"/>
                    </svg>
                    Speak Answer
                </button>
                <button class="stop-btn" id="stop-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                        <path d="M5 3.5h6A1.5 1.5 0 0 1 12.5 5v6a1.5 1.5 0 0 1-1.5 1.5H5A1.5 1.5 0 0 1 3.5 11V5A1.5 1.5 0 0 1 5 3.5z"/>
                    </svg>
                    Stop Speaking
                </button>
            </div>
            
            <div class="status" id="status"></div>
        </div>
        
        <footer>
            <p>Feature Engineering & Selection Knowledge Base | Text-to-Speech Enabled</p>
        </footer>
    </div>

    <script>
        // Data containing all questions and answers
        const data = {
            "1": {
                "title": "1. Feature Engineering",
                "content": "Feature engineering is the process of using domain knowledge to create new features (variables) from existing data to improve the performance of machine learning models. It is a crucial step as the performance of a model is heavily dependent on the quality of the features provided.\n\nKey aspects include:\n\n- Creation: Generating new features, e.g., creating 'FamilySize' from 'SibSp' and 'Parch' in the Titanic dataset.\n\n- Transformation: Modifying existing features, such as converting a 'Date' string into separate 'Day', 'Month', and 'Year' components, or extracting the title from a 'Name' field (Mr., Mrs., Miss).\n\n- Aggregation: Summarizing data, like calculating the average spending per customer.\n\n- Handling Missing Values: Imputing missing data using mean, median, mode, or more advanced techniques like K-Nearest Neighbors.\n\nThe goal is to provide the model with more informative and relevant inputs, which can lead to significantly better predictive accuracy."
            },
            "2": {
                "title": "2. Feature Selection",
                "content": "Feature selection is the process of automatically or manually selecting a subset of the most relevant features from the original dataset for model construction. It is essential to avoid the 'curse of dimensionality' and build simpler, faster, and more interpretable models.\n\nBenefits include:\n\n- Reduces Overfitting: Less redundant data means less opportunity for the model to learn noise.\n\n- Improves Accuracy: By removing irrelevant features, the model can focus on the true signals.\n\n- Reduces Training Time: Fewer features mean fewer computations.\n\n- Enhances Model Interpretability: It's easier to understand a model built with 10 features than one with 1000.\n\nFeature selection methods are broadly classified into three categories: Filter methods, Wrapper methods, and Embedded methods."
            },
            "3": {
                "title": "3. Filter Based Approach",
                "content": "The Filter based approach is a feature selection method that selects features based on their intrinsic statistical properties, independent of any machine learning model. It uses a statistical measure (a 'filter') to score each feature and select the top-ranked ones.\n\nCharacteristics:\n\n- Model Agnostic: It is not tied to a specific learning algorithm, making it computationally cheap and fast.\n\n- Univariate: Typically, it evaluates each feature in isolation, ignoring feature dependencies.\n\n- Common Statistical Measures: Includes Correlation Coefficient, Information Gain, Chi-Square Test, and Fisher Score.\n\nProcess:\n\n1. Calculate a score for each feature.\n\n2. Rank the features based on their scores.\n\n3. Select the top-k features or all features above a certain threshold.\n\nWhile efficient, its main drawback is that it may fail to select the optimal feature subset if features are correlated, as it does not consider feature interactions."
            },
            "4": {
                "title": "4. Information Gain",
                "content": "Information Gain is a filter-based feature selection metric commonly used for classification tasks. It measures how much 'information' a feature provides about the target class. It is based on the concept of Entropy from information theory.\n\nExplanation:\n\n- Entropy: Measures the impurity or uncertainty in a dataset. A high entropy means the data is mixed (50% class A, 50% class B), while low entropy means the data is pure (100% class A).\n\n- Information Gain: It is the reduction in entropy achieved by partitioning the data according to a feature. A feature that perfectly separates the classes will have a high Information Gain.\n\nMathematically, IG(T, A) = Entropy(T) - Entropy(T|A), where T is the target and A is the feature.\n\nFeatures with high Information Gain are considered more important for predicting the target variable, as they bring the most order to the data."
            },
            "5": {
                "title": "5. Chi-Square Test",
                "content": "The Chi-Square (χ²) test is a statistical filter method used for feature selection in categorical datasets. It assesses the independence between a feature and the target class. The fundamental hypothesis is that the feature and the target are independent.\n\nProcess:\n\n- For each categorical feature, a contingency table is built against the target class.\n\n- The Chi-Square statistic is calculated, which measures the divergence of the observed frequencies from the frequencies expected if the feature and target were independent.\n\n- A high Chi-Square value indicates that the observed and expected counts are significantly different, leading to the rejection of the null hypothesis. This means the feature is dependent on the target and is thus relevant for classification.\n\nIt is a powerful method for selecting categorical features but is not suitable for continuous data unless they are first discretized (binned)."
            },
            "6": {
                "title": "6. Fisher Score (and the Fisher Bone Method)",
                "content": "The Fisher Score is a filter-based feature selection technique that selects features which best separate the data points from different classes. It does this by maximizing the ratio of the between-class variance to the within-class variance for each feature.\n\nIntuition:\n\nA good feature should have:\n\n- High between-class variance: The means of different classes should be far apart.\n\n- Low within-class variance: The data points within the same class should be close to each other.\n\nCalculation: For a feature, the Fisher Score is calculated as: (SB / SW), where SB is between-class scatter and SW is within-class scatter.\n\nA higher Fisher Score indicates a feature with better discriminatory power. The term 'Fisher Bone Method' is likely a mishearing or misspelling of the Fisher-Rao method or simply refers to the core ('bone') principle of the Fisher Score."
            },
            "7": {
                "title": "7. Handling Missing Values",
                "content": "Handling missing values is a critical step in data preprocessing, as most machine learning algorithms cannot work with incomplete data. The strategy depends on the nature and amount of the missing data.\n\nCommon Techniques:\n\n- Deletion:\n  - Listwise Deletion: Removing entire rows with any missing value. Suitable only if the data is large and the missing values are random.\n\n- Imputation (Filling in values):\n  - For Numerical Data: Use mean, median, or mode.\n  - For Categorical Data: Use the mode (most frequent category).\n  - Advanced Methods: Use algorithms like K-Nearest Neighbors (KNN) or regression models to predict and fill missing values based on other features.\n\n- Flagging: Creating an additional binary feature to indicate whether the value was missing, which can sometimes be informative for the model.\n\nChoosing the right method is essential to prevent bias and maintain the integrity of the dataset."
            },
            "8": {
                "title": "8. Wrapper Method",
                "content": "Wrapper methods are a category of feature selection that uses the performance of a specific machine learning model to evaluate the quality of a feature subset. They 'wrap' around a predictive model and use its performance as the objective function.\n\nCommon Approaches:\n\n- Forward Selection: Starts with no features and iteratively adds the feature that most improves the model's performance.\n\n- Backward Elimination: Starts with all features and iteratively removes the least significant feature.\n\n- Recursive Feature Elimination (RFE): A popular form of backward elimination that recursively fits the model and removes the weakest features.\n\nAdvantage: They are very effective and can capture feature interactions.\n\nDisadvantage: They are computationally very expensive and prone to overfitting, especially with large feature sets, as they require training a model for every candidate feature subset."
            },
            "9": {
                "title": "9. Exhaustive Feature Selection",
                "content": "Exhaustive Feature Selection is a wrapper method that aims to find the best subset of features by evaluating all possible feature combinations. It trains and tests a machine learning model for every single possible subset of features.\n\nProcess:\nFor a dataset with n features, it evaluates 2^n - 1 possible subsets (excluding the empty set). The subset that results in the highest model performance (e.g., accuracy) is selected as the optimal feature set.\n\nAdvantage: It guarantees finding the best possible subset of features for a given model and performance metric.\n\nDisadvantage: It is computationally prohibitive and infeasible for even a moderately large number of features (e.g., 20 features would require 1,048,575 model evaluations), making it impractical for most real-world scenarios."
            },
            "10": {
                "title": "10. Recursive Feature Elimination (RFE)",
                "content": "Recursive Feature Elimination (RFE) is a popular and efficient wrapper-style feature selection method. It works by recursively removing the least important features and building a model on the remaining features.\n\nSteps:\n\n1. Train a model (like a linear model or SVM that can provide feature importance) on the entire set of features.\n\n2. Rank the features based on their importance (e.g., coefficients in linear models).\n\n3. Remove the least important feature(s) from the current set.\n\n4. Repeat steps 1-3 with the reduced feature set until the desired number of features is reached.\n\nRFE is computationally more feasible than exhaustive search and effectively finds a high-performing subset of features. It is widely used with models like Support Vector Machines (SVM) and Logistic Regression."
            },
            "11": {
                "title": "11. Project: Titanic Dataset Feature Engineering",
                "content": "In the Titanic dataset, effective feature engineering can significantly boost model performance by creating more meaningful predictors from raw data.\n\n1. Family and Sibling Features:\nThe raw features 'SibSp' (siblings/spouse) and 'Parch' (parents/children) can be combined.\n\n- FamilySize = SibSp + Parch + 1 (including the passenger themselves). This captures the total family size on board.\n\n- IsAlone: A binary feature (1 if FamilySize == 1, else 0). This can be a strong indicator, as solo travelers might have had a different survival rate.\n\n2. Ticket Price & Class Features:\nWhile 'Pclass' (ticket class) is a direct indicator of socio-economic status, combining it with 'Fare' (ticket price) can reveal inconsistencies or more nuanced information.\n\n- Fare per Person = Fare / FamilySize: This normalizes the fare, as a single ticket might have been purchased for an entire family. A high fare per person in a lower class might be a significant indicator.\n\n- Analyzing Discrepancies: A passenger in Pclass=1 with a very low fare, or in Pclass=3 with a very high fare, could be engineered into a feature flag, potentially indicating data errors or special circumstances.\n\nThese engineered features help the model capture complex relationships that the original raw features alone might miss."
            },
            "12": {
                "title": "12. Encoding Categorical Variables",
                "content": "Most machine learning algorithms require numerical input. Encoding is the process of converting categorical variables (text or categories) into a numerical format.\n\nCommon Techniques:\n\n- Label Encoding: Assigns a unique integer to each category (e.g., Red=0, Green=1, Blue=2). Suitable for ordinal data (where order matters, like 'Low', 'Medium', 'High').\n\n- One-Hot Encoding: Creates new binary columns for each category. A value of 1 indicates the presence of that category. This is preferred for nominal data (where no order exists, like 'France', 'Germany', 'Spain') as it avoids imposing a false ordinal relationship.\n\n- Target Encoding: Replaces a category with the mean of the target variable for that category. Can be powerful but is prone to overfitting.\n\nChoosing the correct encoding method is vital to prevent the model from learning incorrect relationships from the data."
            },
            "13": {
                "title": "13. Embedding",
                "content": "Embedding is an advanced technique for representing high-dimensional categorical data (like words or user IDs) as low-dimensional, dense vectors of real numbers. Unlike one-hot encoding which creates sparse, high-dimensional vectors, embeddings are learned by the model and capture semantic meaning.\n\nKey Properties:\n\n- Dimensionality Reduction: They represent categories in a much lower-dimensional space.\n\n- Semantic Capture: Similar categories have similar vector representations. For example, in word embeddings, the vectors for 'king' and 'queen' are closer to each other than to the vector for 'apple'.\n\n- Learned Representation: The values of the embedding vectors are parameters that are learned during the model training process.\n\nEmbeddings are a cornerstone of modern NLP (e.g., Word2Vec) and recommendation systems, allowing models to understand complex relationships between categorical entities."
            },
            "14": {
                "title": "14. Dimensionality Reduction",
                "content": "Dimensionality reduction is the process of reducing the number of random variables (features) under consideration by obtaining a set of principal variables. It helps in combating the 'curse of dimensionality,' which can lead to overfitting and high computational costs.\n\nIt serves two main purposes:\n\n- Visualization: Projecting high-dimensional data onto 2D or 3D planes for human interpretation.\n\n- Feature Extraction: Creating a new, smaller set of features that captures most of the important information from the original data.\n\nThe two primary techniques are:\n\n- Principal Component Analysis (PCA): An unsupervised method that finds directions of maximum variance.\n\n- Linear Discriminant Analysis (LDA): A supervised method that finds directions that maximize the separation between classes."
            },
            "15": {
                "title": "15. Principal Component Analysis (PCA)",
                "content": "Principal Component Analysis (PCA) is an unsupervised, linear dimensionality reduction technique. Its goal is to transform the original features into a new set of uncorrelated features called Principal Components, which are ordered by the amount of variance they capture from the data.\n\nHow it works:\n\n1. Standardize the data (mean=0, variance=1).\n\n2. Compute the covariance matrix to understand feature relationships.\n\n3. Calculate the eigenvectors (principal components) and eigenvalues of this matrix. The eigenvectors define the directions of the new feature space, and the eigenvalues indicate the magnitude of variance along these directions.\n\n4. Select the top-k eigenvectors with the highest eigenvalues to form the new, lower-dimensional dataset.\n\nPCA is excellent for noise reduction and visualization but does not consider class labels, so it is not always optimal for classification tasks."
            },
            "16": {
                "title": "16. Linear Discriminant Analysis (LDA)",
                "content": "Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique primarily used for classification tasks. Unlike PCA, which focuses on maximizing variance, LDA finds a feature subspace that maximizes the separation between multiple classes.\n\nObjective:\nLDA seeks to project the data onto a lower-dimensional space that achieves:\n\n- Maximum between-class separation: The distance between the means of different classes should be as large as possible.\n\n- Minimum within-class variance: The data points of the same class should be as close together as possible.\n\nThis results in a projection where classes are as distinct and compact as possible, which often leads to better classification performance than PCA when the class labels are known. LDA is both a dimensionality reducer and a classifier."
            },
            "17": {
                "title": "17. Bagging and Boosting",
                "content": "Bagging and Boosting are two powerful ensemble learning techniques that combine multiple weak learners (e.g., decision trees) to create a single strong learner.\n\nBagging (Bootstrap Aggregating):\n\n- Concept: Trains multiple models in parallel on different random subsets of the training data (sampled with replacement).\n\n- Goal: To reduce variance and prevent overfitting.\n\n- Combining Predictions: For classification, it uses a majority vote; for regression, it uses an average.\n\n- Prime Example: Random Forest, which is an ensemble of decision trees trained with bagging.\n\nBoosting:\n\n- Concept: Trains models sequentially, where each new model focuses on correcting the errors made by the previous ones.\n\n- Goal: To reduce bias and create a strong learner from a sequence of weak ones.\n\n- Combining Predictions: Models are weighted based on their performance; more accurate models have a higher say.\n\n- Prime Examples: AdaBoost, Gradient Boosting Machines (GBM), and XGBoost.\n\nIn summary, Bagging reduces variance, while Boosting reduces bias."
            }
        };

        // DOM elements
        const questionSelect = document.getElementById('question-select');
        const answerTitle = document.getElementById('answer-title');
        const answerContent = document.getElementById('answer-content');
        const speakBtn = document.getElementById('speak-btn');
        const stopBtn = document.getElementById('stop-btn');
        const status = document.getElementById('status');

        // Speech synthesis
        let speech = null;

        // Event listener for dropdown change
        questionSelect.addEventListener('change', function() {
            const selectedValue = this.value;
            
            if (selectedValue && data[selectedValue]) {
                answerTitle.textContent = data[selectedValue].title;
                answerContent.textContent = data[selectedValue].content;
                status.className = 'status';
            } else {
                answerTitle.textContent = "Please select a topic to view its explanation";
                answerContent.textContent = "";
                status.className = 'status';
            }
        });

        // Event listener for speak button
        speakBtn.addEventListener('click', function() {
            const selectedValue = questionSelect.value;
            
            if (!selectedValue || !data[selectedValue]) {
                status.textContent = "Please select a topic first";
                status.className = 'status no-answer';
                return;
            }
            
            // Stop any ongoing speech
            if (speech) {
                window.speechSynthesis.cancel();
            }
            
            // Create new speech
            speech = new SpeechSynthesisUtterance();
            speech.text = data[selectedValue].title + ". " + data[selectedValue].content;
            speech.rate = 0.9;
            speech.pitch = 1;
            speech.volume = 1;
            
            // Update status
            status.textContent = "Speaking...";
            status.className = 'status speaking';
            
            // Handle speech end
            speech.onend = function() {
                status.textContent = "Speech completed";
                status.className = 'status';
            };
            
            // Start speaking
            window.speechSynthesis.speak(speech);
        });

        // Event listener for stop button
        stopBtn.addEventListener('click', function() {
            if (window.speechSynthesis.speaking) {
                window.speechSynthesis.cancel();
                status.textContent = "Speech stopped";
                status.className = 'status';
            }
        });
    </script>
</body>
</html>
